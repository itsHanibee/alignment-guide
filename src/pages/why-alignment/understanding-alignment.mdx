import Promise from "@/components/elements/Promise";
import RiskTable from "@/components/elements/RiskTable";
import StyledText from "@/components/elements/StyledText";
import Toggle from "@/components/elements/Toggle";

# Understanding AI Alignment

<Promise>
What is AI “alignment,” and how does it differ from “safety” and “control”?

**After reading this, you will understand what “alignment” is and how these terms are related.**

</Promise>

## Definitions

<u>AI safety</u> is the term for _all_ things “reducing risks posed by AI” (misuse,
reliability, privacy, etc.). It may be helpful to visualize AI safety in four quadrants:

<div class="mt-4">
  <RiskTable />
</div>

This guide is about the upper righthand square, where things go wrong by accident (and by default). When we think about preventing this long-term accident risk, we think about solving the <u>AI control</u> problem.

<u>AI control</u> is about ensuring that AI systems try to do the right thing, and
if they don’t, that they don’t competently pursue the wrong thing.

<div>
  <u>AI alignment</u> is an approach to the <u>AI control</u> problem. More specifically,
  the **alignment** problem is about <StyledText highlight>building AI systems that are trying to
    do what their operator wants them to do</StyledText>. In other words, AI alignment aims to
  solve the control problem by making sure the AI does the right thing to begin with,
  without considering approaches like containing, manipulating, or outsmarting the
  AI.
</div>

## Alignment = _motives_ of AI, not their knowledge or ability

1. So “aligned” ≠ “perfect.” The AI can misunderstand instructions or the operator’s wants, predict consequences of actions incorrectly, or even **create an unaligned AI** itself.
2. Current systems like GPT-4 can’t take over the world but are still **misaligned**. (They will often do things their designers didn’t want, such as lie or be offensive.)

## Alignment ≠ human values

The alignment problem is commonly misunderstood as “aligning the AI’s values/preferences with humans.”

<Toggle title="But the type of alignment we are discussing is not so much about aligning the AI to human values, but to an operator’s intentions (the precise term for this is “intent alignment”).">
  Intent alignment is significantly narrower, as it only concerns aligning an AI to its operators intent.
  
 That is, even if an aligned system is created, an operator can use it to do “harmful” things, if that’s what they intend. (Although addressing these misuse cases are arguably less urgent than the long-term accident risk that we mentioned above).
</Toggle>

---

<Toggle title="Sources / dig deeper">
  The definitions above come from **our interpretations** of Paul Christiano’s writing. If you wish to dig deeper into these definitions, he has very precise and thoughtful blog posts:
  - [**AI “safety” vs “control” vs “alignment”**](https://ai-alignment.com/ai-safety-vs-control-vs-alignment-2a4b42a863cc) _(Nov 2016)_: his initial definition attempt
  - [**Clarifying “AI alignment”**](https://ai-alignment.com/clarifying-ai-alignment-cec47cd69dd6) _(April 2018):_ his latest definition of “AI alignment”
  - [**AI alignment is distinct from its near-term applications**](https://ai-alignment.com/ai-alignment-is-distinct-from-its-near-term-applications-81300500ad2e) _(Dec 2022)_: contains his best definition + an important comment on alignment misconceptions
</Toggle>
