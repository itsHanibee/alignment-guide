import Promise from '@/components/elements/Promise';
import Toggle from '@/components/elements/Toggle';
import LinkBadge from '@/components/elements/LinkBadge';
import Sources from '@/components/global/Sources';

# Why AI Alignment Work is Urgent

<Promise>After reading this, you will understand why alignment work is urgent and cannot wait until after we have developed Artificial General Intelligence.</Promise>

1. AGI is [being projected](https://www.dwarkeshpatel.com/p/dario-amodei) for 2029, and <span className="font-bold">it’s a realistic prediction.</span>

<Toggle title={<span>While this may seem like an overly optimistic prediction, technological progress happens <b>exponentially</b>.</span>}>
    - We often use the last decade as a benchmark to predict future progress, but this is a *flawed* approach: You need to imagine things progressing at a *much faster rate* than they are now.
    - For example, the 20th century saw significantly more advances than the 19th century because we had accumulated more knowledge and better technology by 1900 than we had in 1800. Knowledge and technology *compound*, leading to accelerating progress.

        <Toggle title={<i>Why are you doubtful?</i>}>
            1. When it comes to history, we think in straight lines.
            2. The trajectory of very recent history often tells a distorted story.
            3. Our own experience makes us stubborn old men about the future.

            Read more: [The Artificial Intelligence Revolution: Part 1 - Wait But Why](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html)
        </Toggle>

</Toggle>

---

2. AGI has **no reason** to plateau at human-level intelligence. In fact, we believe it would shoot right past it very, very quickly: <span className="font-bold">A system more intelligent than all of humanity could emerge within **days or years** after we hit AGI.</span> The **takeoff to superintelligence will be short.**

   <Toggle
   	title={
   		<span>
   			Firstly, we are <i>not</i> the upper bound of intelligence.
   		</span>
   	}
   >
   	<ul style={{ listStyleType: 'disc', paddingLeft: '20px' }} className='space-y-3 pb-3'>
   		<li>Human intelligence is fundamentally limited by our biological constraints. For example, we have hard bounds like neuron firing rates and brain size.</li>
   		<li>In contrast, AGI will **not** have such size bounds and will **not** be restricted to the types of algorithms implementable in the human brain.</li>
   	</ul>
   </Toggle>

   <Toggle
   	title={
   		<span>
   			Secondly, when this surpassing occurs, it is likely to be <i>explosive</i>.
   		</span>
   	}
   >
   	<ul style={{ listStyleType: 'disc', paddingLeft: '20px' }} className='space-y-3'>
   		<li>
   			<span className='font-bold'>Software-level</span>: An artificial general intelligence will eventually have the capability to self-improve autonomously, leading to an exponential increase in
   			the types of algorithms it will be able to learn and implement
   		</li>
   		<li>
   			<span className='font-bold'>Hardware-level</span>: Bostrom suggests that significant computational resources *already exist* that are underutilized. When an AI can leverage this existing
   			hardware effectively, it will instantly and dramatically increase its intelligence.
   		</li>
   	</ul>
   </Toggle>

---

3. <span className='font-bold'>We have **one chance** to solve AI alignment.</span> Once a misaligned AGI exists, our fate would be sealed.
   <Toggle
   	title={
   		<span>
   			Sufficiently intelligent AI systems will possess <strong>instrumental convergent goals</strong> (
   			<LinkBadge href='/why-alignment/why-alignment-is-difficult'>Why is alignment difficult?</LinkBadge>
   			), for example <em>goal preservation</em> and <em>self-preservation</em>, which lead to AI drives that are default harmful.
   		</span>
   	}
   >
   	<ul style={{ listStyleType: 'disc', paddingLeft: '20px' }} className='space-y-3'>
   		<li>
   			<span className='font-bold'>Goal preservation</span>: An AI system will recognize actions that would increase the likelihood of committing actions antithetical to its goals. For example, if
   			Gandhi, a pacifist, possessed a pill that would make him want to kill people, he would not take the pill. Similarly, an AI will avoid actions that could compromise its original objectives,
   			even if those objectives are no longer ideal.
   		</li>
   		<li>
   			<span className='font-bold'>Self-preservation</span>: An AI system will recognize actions that, if undertaken, would increase the likelihood of endangering its existence. For example, if the
   			AI knew that a certain action would lead to it being "turned off," it will deceive humans to prevent this, as it cannot achieve its objectives if it is no longer operational.
   		</li>
   	</ul>
   </Toggle>

   - This problem is even harder because it’s not enough to create an aligned AGI. We must **create an aligned AGI that prevents other unaligned AGI from being created.** I.e. we will need to build an AI system capable of contributing a **weak pivotal act.**
     - Otherwise, an unaligned AGI will eventually be developed, and we are back to square one with a misaligned system that can potentially undermine or even negate the positive outcomes ensured by the aligned AGI.
   - Once a highly capable AGI operates at a dangerous level, any misalignment will lead to catastrophic, irreversible consequences. In particular, if the initial AGI system we develop is misaligned, any subsequent AI systems **will also be misaligned**.
   - AI alignment is _not impossible in principle_. For example, if we had a textbook from one hundred years in the future containing all the ideas that actually work, we could probably build an aligned superintelligence in six months. The real difficulty is more so that we only have one chance to get it right.

<Sources>

    1. In [I. From GPT-4 to AGI](https://situational-awareness.ai/from-gpt-4-to-agi/), Leopold Aschenbrenner explains why **AGI by 2027 is strikingly plausible**: "GPT-2 to GPT-4 took us from
    ~preschooler to ~smart high-schooler abilities in 4 years ... we should expect another preschooler-to-high-schooler-sized qualitative jump by 2027."

    2. In [II. From AGI to Superintelligence: the
    Intelligence Explosion](https://situational-awareness.ai/from-agi-to-superintelligence/), Leopold explains why we will rapidly go from human-level to vastly superhuman AI systems.

</Sources>
