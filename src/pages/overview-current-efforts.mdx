import Promise from "@/components/elements/Promise";
import {
  Accordion,
  AccordionContent,
  AccordionItem,
  AccordionTrigger,
} from "@/components/ui/accordion";

# Overview of current efforts

<Promise>
  There are many great organizations conducting significant alignment research.
  Here are a few of them and a brief overview of their work.

**After reading this, you will have an overview of a few research directions
at these companies and know where to go to read more.**

</Promise>

<div class="mt-1">
  <Accordion type="single" collapsible className="w-full">
    <AccordionItem value="item-1">
      <AccordionTrigger>
        **OpenAI** is working on Weak-to-strong generalization (W2SG) and
        have also conducted prior work on understanding Reward Model
        Overoptimization.
      </AccordionTrigger>
      <AccordionContent>
            - **[W2SG](https://openai.com/index/weak-to-strong-generalization/)** is the idea of using a smaller, less intelligent model
            to align a larger, more powerful model while still extracting the
            latent knowledge of the more powerful model.
            - **[Reward Model Overoptimization](https://arxiv.org/abs/2210.10760)** examines how models may focus
            on optimizing a proxy measure instead of achieving the actual
            objective (i.e. Goodhart’s law). OpenAI has conducted research on
            the scaling laws associated with this phenomenon.
            - **[Preparedness Framework](https://openai.com/preparedness/):** OpenAI's "processes to track, evaluate, forecast, and protect against catastrophic risks posed by increasingly powerful models."
      </AccordionContent>
    </AccordionItem>
    <AccordionItem value="item-2">
      <AccordionTrigger>
        **Anthropic** is working on understanding Sleeper Agents, Sparse
        Autoencoders (SAEs), and Circuit Analysis.
      </AccordionTrigger>
      <AccordionContent>

            - **[Sleeper Agents](https://www.anthropic.com/news/sleeper-agents-training-deceptive-llms-that-persist-through-safety-training):** Humans can act deceptively, appearing helpful
            while secretly pursuing hidden objectives. Anthropic has explored
            whether current safety techniques can eliminate learned deceptive
            strategies in AI systems. In particular, they have found that
            removing deceptive behavior with current techniques is not always
            possible.

            - **[Sparse Autoencoders and Monosemanticity](https://transformer-circuits.pub/2023/monosemantic-features):** Many neurons in
            neural networks are polysemantic, responding to mixtures of
            seemingly unrelated inputs. This complexity makes interpretability
            challenging, as it becomes difficult to understand the function of
            each component within the network. Sparse Autoencoders generate
            learned features from a trained model, offering more monosemantic
            units of analysis than the model's neurons themselves.

            - **[Responsible Scaling Policy](https://www.anthropic.com/news/anthropics-responsible-scaling-policy):** Anthropic details their protocols surrounding developing increasingly capable AI systems.

      </AccordionContent>
    </AccordionItem>
    <AccordionItem value="item-3">
      <AccordionTrigger>
        **Google Deepmind** is working on Activation Steering and their
        Frontier Safety Framework.
      </AccordionTrigger>
      <AccordionContent>
            - **[Activation Steering](https://www.alignmentforum.org/posts/C5KAZQib3bzzpeyrg/progress-update-1-from-the-gdm-mech-interp-team-full-update)** is a technique whereby one takes
            activation vectors from the residual stream of a model on specific
            prompts and adds these vectors to the residual stream on a different
            prompt. This manipulation causes the output of the second prompt to
            inherit properties from the first, enabling one to steer the model’s
            outputs toward desired behaviors or themes.
            - **[Frontier Safety Framework](https://deepmind.com/blog/democratizing-access-to-large-scale-language-models-with-frontier-safety-framework)**: Deepmind released a specification
            of protocols for detecting “Critical Capability Levels” of models
            and how to mitigate the risks posed when models surpass these
            capabilities.
      </AccordionContent>
    </AccordionItem>

  <AccordionItem value="item-4">
    <AccordionTrigger>
        **Academia**
    </AccordionTrigger>
    <AccordionContent>
        Here is a non-comprehensive list of alignment work being done in academia:
        - Interpretability
            - In **["Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task"](https://arxiv.org/pdf/2210.13382)**, a group of researchers at Harvard, MIT, and Northeastern to extend partial game transcripts of Othello with legal moves. They show that although the model has no knowledge of the game’s rules, it forms an internal representation of the board state, which it then uses to generate legal moves.
            - **["Finding Neurons in a Haystack: Case Studies with Sparse Probing"](https://arxiv.org/pdf/2305.01610)** introduces *sparse probing* to understand better how human-interpretable features are represented in activations of LLMs. The paper uses k-sparse probes (linear classifiers with at most k non-zero coefficients) to identify specific neurons or sparse sets of neurons responsible for encoding particular features.
        - Surveys
            - A nice survey of many different areas of alignment research is given in **["AI Alignment: A Comprehensive Survey"](https://alignmentsurvey.com/uploads/AI-Alignment-A-Comprehensive-Survey.pdf)**. There, they decompose the alignment problem into two components: aligning AI systems via appropriate training, understanding if systems are aligned, and then governing them.
    </AccordionContent>
    </AccordionItem>

    <AccordionItem value="item-5">
      <AccordionTrigger>
        **Other Organizations**
      </AccordionTrigger>
      <AccordionContent>
        - **[Center for AI Safety (CAIS)](https://www.safe.ai/work)**: CAIS researches ways to reduce societal-scale risks caused by AI and provides guidance to policymakers on effective strategies for governing AI deployment and ensuring its safe integration into society.
        - **[Machine Intelligence Research Institute (MIRI)](https://intelligence.org/)**: Focused on doing research to make sure superintelligent models have a positive impact on humanity. They’ve done work on Corrigibility, i.e., AI systems that assist and do not resist corrective interventions (note that by default, AI systems are not corrigible (see **[Why is alignment difficult](https://www.notion.so/Why-is-alignment-difficult-385f815223004c148a1e6879afe10bcf?pvs=21)**).
        - **[US AI Safety Institute](https://www.nist.gov/aisi)**: Led by Paul Christiano, the institute works on advancing research and measurement science for AI safety, conducting safety evaluations of models and systems, and developing guidelines for evaluations and risk mitigations, including content authentication and the detection of synthetic content.
      </AccordionContent>
    </AccordionItem>
  </Accordion>
</div>
