import Promise from '@/components/elements/Promise';
import Toggle from '@/components/elements/Toggle';
import LinkBadge from '@/components/elements/LinkBadge';

# Next steps
<Promise>
What now? What can you do to contribute and work on alignment?

<b>Here are our thoughts.</b>
</Promise>

Here, we give a few different ways to begin onboarding to work on alignment.

## Current research directions and future work

One high-value thing to do would be to take one of the ideas listed in a "future work" section on <LinkBadge href="/research-directions">Research Directions</LinkBadge> and start working on it. Many of the current methods in alignment research are still in their infancy, and there is a lot of low-hanging fruit to pick that will improve current methods and suggest which methods are worth working on. On that page, we’ve also linked relevant literature and sources that can be used to help you with the problem.

In particular, we view W2SG as a worthwhile path to pursue, and we highly recommend investigating ways to contribute to this research direction.

## Novel alignment methods

Alignment is still in its early stages, and it’s unclear whether our current methods (or slight modifications thereof) will be sufficient to align AGI. Hence, spending time developing entirely new paradigms for alignment is also extremely high-value. 

If you have specific thoughts here, writing up your ideas clearly and getting feedback from others (e.g., by posting to places like AlignmentForum) is worth it.

## Measurements and benchmarks

There is currently no clear measure of " success“ or even “progress” for AI alignment; it is incredibly difficult to understand how much meaningful progress we are making. 

A good proposal for such a metric that covers the engineering, ethical, and/or governmental difficulties of super alignment could be very useful in:

1) Ensuring we work on what’s important, and

2) Increasing the field's motivation and tractability.

For example, a high-quality “[MMLU](https://arxiv.org/abs/2009.03300) for Alignment” would greatly contribute to alignment research.

## Dipping your toes in mech interpretability

<Toggle title={<div>Another option if you would like to leap directly into technical work would be to select a problem from Neel Nanda’s <a href="https://www.alignmentforum.org/posts/LbrPTJ4fmABEdEnLf/200-concrete-open-problems-in-mechanistic-interpretability">200</a> Open Problems in mechanistic interpretability. Progress in interpretability is likely to be extremely helpful in the future for alignment, and Neel’s list is a great way to get exposed to interesting problems in the area.</div>}>
    - Neel also has great thoughts on how to [get started in mech interpretability](https://www.neelnanda.io/mechanistic-interpretability/getting-started).
    - Some of the problems there are likely solved, and that’s okay! A great corollary of working on a problem or two from that list is the upskilling that it will provide.
</Toggle>
